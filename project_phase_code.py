# -*- coding: utf-8 -*-
"""Project_phase_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D1fKudxkOJbQvs-n0PU2xCBrbLmv39UU

Upload the Dataset
"""

from google.colab import files
uploaded = files.upload()

"""Load the Dataset

"""

import pandas as pd
# Read the dataset
df = pd.read_csv('credit_card_fraud_dataset.csv')

"""Data Exploration"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
file_path = "credit_card_fraud_dataset.csv"  # Update path as needed
df = pd.read_csv(file_path)

# ----- Basic Info -----
print("üîπ Shape of dataset:", df.shape)
print("\nüîπ Column Data Types:\n", df.dtypes)
print("\nüîπ Missing Values:\n", df.isnull().sum())

# ----- Descriptive Statistics -----
print("\nüîπ Descriptive Statistics:\n", df.describe())

# ----- Class Distribution -----
if 'IsFraud' in df.columns:
    print("\nüîπ Class Distribution:\n", df['IsFraud'].value_counts())

    # Plot class distribution
    plt.figure(figsize=(6, 4))
    sns.countplot(x='IsFraud', data=df)
    plt.title('Class Distribution (IsFraud)')
    plt.xlabel('IsFraud')
    plt.ylabel('Count')
    plt.show()

# ----- Correlation Matrix -----
numeric_df = df.select_dtypes(include=['int64', 'float64'])
plt.figure(figsize=(8, 6))
sns.heatmap(numeric_df.corr(), annot=True, fmt=".2f", cmap="coolwarm")
plt.title('Correlation Matrix')
plt.show()

# ----- Transaction Amount Distribution -----
plt.figure(figsize=(10, 5))
sns.histplot(df['Amount'], bins=50, kde=True)
plt.title('Transaction Amount Distribution')
plt.xlabel('Amount')
plt.ylabel('Frequency')
plt.show()

# ----- Fraud vs Amount -----
if 'IsFraud' in df.columns:
    plt.figure(figsize=(10, 5))
    sns.boxplot(x='IsFraud', y='Amount', data=df)
    plt.title('Transaction Amount by Fraud Status')
    plt.xlabel('IsFraud')
    plt.ylabel('Amount')
    plt.show()

# Optional: Convert and parse date column if present
if 'TransactionDate' in df.columns:
    df['TransactionDate'] = pd.to_datetime(df['TransactionDate'], errors='coerce')
    df.set_index('TransactionDate', inplace=True)

    # Plot number of transactions over time
    df['IsFraud'].resample('D').count().plot(title='Transactions Over Time', figsize=(12, 4))
    plt.ylabel('Transaction Count')
    plt.show()

"""Check for Missing Values and Duplicates"""

import pandas as pd

# Load dataset
file_path = "credit_card_fraud_dataset.csv"  # Update path as needed
df = pd.read_csv(file_path)

# ----- Check for Missing Values -----
print("üîπ Missing Values per Column:")
missing_values = df.isnull().sum()
print(missing_values)

# Optional: Show only columns with missing values
missing = missing_values[missing_values > 0]
if not missing.empty:
    print("\nüîπ Columns with Missing Values:")
    print(missing)
else:
    print("\n‚úÖ No missing values found.")

# ----- Check for Duplicates -----
duplicate_count = df.duplicated().sum()
print(f"\nüîπ Number of Duplicate Rows: {duplicate_count}")

# Optional: Display duplicate rows
if duplicate_count > 0:
    print("\nüîπ Duplicate Rows Preview:")
    print(df[df.duplicated()].head())
else:
    print("‚úÖ No duplicate rows found.")

"""Visualize a Few Features"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
file_path = "credit_card_fraud_dataset.csv"  # Update path as needed
df = pd.read_csv(file_path)

# Optional: Set a style
sns.set(style="whitegrid")

# ----- Visualize Transaction Amount Distribution -----
plt.figure(figsize=(10, 5))
sns.histplot(df['Amount'], bins=50, kde=True, color='skyblue')
plt.title("Transaction Amount Distribution")
plt.xlabel("Amount")
plt.ylabel("Frequency")
plt.show()

# ----- Transaction Amount by Fraud Status -----
if 'IsFraud' in df.columns:
    plt.figure(figsize=(8, 5))
    sns.boxplot(x='IsFraud', y='Amount', data=df, palette="Set2")
    plt.title("Transaction Amount by Fraud Status")
    plt.xlabel("IsFraud")
    plt.ylabel("Amount")
    plt.show()

# ----- Count of Transaction Types -----
if 'TransactionType' in df.columns:
    plt.figure(figsize=(8, 5))
    sns.countplot(x='TransactionType', data=df, palette="Set3")
    plt.title("Transaction Type Counts")
    plt.xlabel("Transaction Type")
    plt.ylabel("Count")
    plt.xticks(rotation=45)
    plt.show()

# ----- Fraud Rate per Transaction Type -----
if 'IsFraud' in df.columns and 'TransactionType' in df.columns:
    plt.figure(figsize=(10, 6))
    fraud_rate = df.groupby('TransactionType')['IsFraud'].mean().sort_values(ascending=False)
    sns.barplot(x=fraud_rate.index, y=fraud_rate.values, palette="coolwarm")
    plt.title("Fraud Rate by Transaction Type")
    plt.xlabel("Transaction Type")
    plt.ylabel("Fraud Rate")
    plt.xticks(rotation=45)
    plt.show()

"""Identify Target and Features"""

import pandas as pd

# Load dataset
file_path = "credit_card_fraud_dataset.csv"  # Update path as needed
df = pd.read_csv(file_path)

# ----- Identify Target Column -----
# Assumption: 'IsFraud' is the target column
target_column = 'IsFraud'

if target_column in df.columns:
    print(f"üéØ Target Variable: {target_column}")
else:
    raise ValueError("Target column 'IsFraud' not found in dataset.")

# ----- Identify Feature Columns -----
feature_columns = [col for col in df.columns if col != target_column]
print("\nüîπ Feature Columns:")
for col in feature_columns:
    print(f"- {col}")

# ----- Split into Features (X) and Target (y) -----
X = df[feature_columns]
y = df[target_column]

# Show basic structure
print(f"\n‚úÖ Features shape: {X.shape}")
print(f"‚úÖ Target shape: {y.shape}")

"""Convert Categorical Columns to Numerical"""

import pandas as pd

# Load dataset
file_path = "credit_card_fraud_dataset.csv"  # Update path as needed
df = pd.read_csv(file_path)

# ----- Identify Categorical Columns -----
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
print("üîπ Categorical Columns Found:", categorical_cols)

# ----- Convert Categorical Columns -----
# Option 1: One-Hot Encoding (Recommended for ML models)
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# ----- Output the Result -----
print(f"\n‚úÖ Converted dataset shape: {df_encoded.shape}")
print(f"üß† New columns created: {set(df_encoded.columns) - set(df.columns)}")

# Optional: Save the encoded dataset
# df_encoded.to_csv("credit_card_fraud_dataset_encoded.csv", index=False)

"""One-Hot Encoding

"""

import pandas as pd

# Load dataset
file_path = "credit_card_fraud_dataset.csv"  # Update path as needed
df = pd.read_csv(file_path)

# ----- Identify Categorical Columns -----
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
print("üîπ Categorical Columns Found:", categorical_cols)

# ----- Apply One-Hot Encoding -----
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# ----- Show Results -----
print(f"\n‚úÖ Original dataset shape: {df.shape}")
print(f"‚úÖ Encoded dataset shape: {df_encoded.shape}")
print("\nüß† New Columns Created (Sample):")
new_columns = list(set(df_encoded.columns) - set(df.columns))
print(new_columns[:10])  # Show only first 10 to keep output clean

# Optional: Save to CSV
# df_encoded.to_csv("credit_card_fraud_dataset_encoded.csv", index=False)

"""Feature Scaling

"""

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load dataset
file_path = "credit_card_fraud_dataset.csv"  # Update path as needed
df = pd.read_csv(file_path)

# ----- Identify Numeric Columns -----
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Optional: Exclude Target and ID columns
columns_to_exclude = ['IsFraud', 'TransactionID']
features_to_scale = [col for col in numeric_cols if col not in columns_to_exclude]

print("üîπ Features to be scaled:", features_to_scale)

# ----- Apply Standard Scaling -----
scaler = StandardScaler()
df_scaled = df.copy()
df_scaled[features_to_scale] = scaler.fit_transform(df[features_to_scale])

# ----- Show Results -----
print("\n‚úÖ Scaled Features Preview:")
print(df_scaled[features_to_scale].head())

# Optional: Save scaled dataset
# df_scaled.to_csv("credit_card_fraud_dataset_scaled.csv", index=False)

"""Train-Test Split"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Load dataset
file_path = "credit_card_fraud_dataset.csv"  # Update path as needed
df = pd.read_csv(file_path)

# ----- Identify Features and Target -----
target_column = 'IsFraud'
X = df.drop(columns=[target_column])
y = df[target_column]

# ----- Perform Train-Test Split -----
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,      # 20% for testing
    random_state=42,    # Ensures reproducibility
    stratify=y          # Maintains class distribution
)

# ----- Show Results -----
print(f"‚úÖ X_train shape: {X_train.shape}")
print(f"‚úÖ X_test shape: {X_test.shape}")
print(f"‚úÖ y_train distribution:\n{y_train.value_counts(normalize=True)}")
print(f"‚úÖ y_test distribution:\n{y_test.value_counts(normalize=True)}")

"""Model Building

"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Load dataset
file_path = "credit_card_fraud_dataset.csv"  # Update path as needed
df = pd.read_csv(file_path)

# ----- Preprocessing -----
# Drop non-numeric or unnecessary columns
columns_to_drop = ['TransactionID', 'TransactionDate', 'Location']
df = df.drop(columns=columns_to_drop, errors='ignore')

# One-Hot Encode categorical columns
df = pd.get_dummies(df, drop_first=True)

# Split features and target
target_column = 'IsFraud'
X = df.drop(columns=[target_column])
y = df[target_column]

# Feature Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ----- Train-Test Split -----
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# ----- Model Building -----
model = LogisticRegression()
model.fit(X_train, y_train)

# ----- Model Evaluation -----
y_pred = model.predict(X_test)

print("‚úÖ Accuracy:", accuracy_score(y_test, y_pred))
print("\nüîç Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nüìä Classification Report:\n", classification_report(y_test, y_pred))

"""Evaluation

"""

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report,
    roc_auc_score,
    roc_curve
)
import matplotlib.pyplot as plt
import seaborn as sns

# Assume y_test and y_pred are already defined
# Example placeholders (you can comment/remove these lines if using real values):
# y_test = [...]
# y_pred = [...]

# ----- Basic Evaluation Metrics -----
print(f"‚úÖ Accuracy:  {accuracy_score(y_test, y_pred):.4f}")
print(f"‚úÖ Precision: {precision_score(y_test, y_pred):.4f}")
print(f"‚úÖ Recall:    {recall_score(y_test, y_pred):.4f}")
print(f"‚úÖ F1 Score:  {f1_score(y_test, y_pred):.4f}")

# ----- Confusion Matrix -----
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Not Fraud", "Fraud"], yticklabels=["Not Fraud", "Fraud"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ----- Classification Report -----
print("\nüìä Classification Report:\n", classification_report(y_test, y_pred))

# ----- ROC AUC Curve -----
# Optional: Get probabilities instead of labels if available
if 'y_pred_proba' in globals():
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    auc = roc_auc_score(y_test, y_pred_proba)

    plt.figure(figsize=(7, 5))
    plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {auc:.2f})")
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
    plt.title("ROC Curve")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend()
    plt.grid(True)
    plt.show()

"""Make Predictions from New Input"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
import numpy as np

# ---- Step 1: Load and Preprocess Training Data ----
# Load dataset
df = pd.read_csv("credit_card_fraud_dataset.csv")

# Drop irrelevant columns if needed
columns_to_drop = ['TransactionID', 'TransactionDate', 'Location']
df = df.drop(columns=columns_to_drop, errors='ignore')

# One-Hot Encode
df_encoded = pd.get_dummies(df, drop_first=True)

# Separate features and target
target_column = 'IsFraud'
X = df_encoded.drop(columns=[target_column])
y = df_encoded[target_column]

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train model
model = LogisticRegression()
model.fit(X_scaled, y)

# ---- Step 2: Make Prediction on New Input ----
# Example new input as a dictionary (update with real values)
new_input = {
    'Amount': 200.0,
    'TransactionType': 'Online',
    'Gender': 'Female',
    # ... add other necessary fields used in the model
}

# Convert to DataFrame
new_df = pd.DataFrame([new_input])

# One-Hot Encode new input
new_df_encoded = pd.get_dummies(new_df)

# Align with training columns
new_df_encoded = new_df_encoded.reindex(columns=X.columns, fill_value=0)

# Scale input
new_input_scaled = scaler.transform(new_df_encoded)

# Make prediction
prediction = model.predict(new_input_scaled)
prediction_proba = model.predict_proba(new_input_scaled)

# Output result
print("üß† Prediction:", "Fraud" if prediction[0] == 1 else "Not Fraud")
print("üìä Probability (Fraud):", prediction_proba[0][1])

"""Convert to DataFrame and Encode

"""

import pandas as pd

# ----- Step 1: Sample New Input -----
new_input = {
    'Amount': 200.0,
    'TransactionType': 'Online',
    'Gender': 'Female',
    # Add all required fields used in the model
}

# ----- Step 2: Convert Dictionary to DataFrame -----
new_df = pd.DataFrame([new_input])
print("‚úÖ New Input as DataFrame:")
print(new_df)

# ----- Step 3: One-Hot Encode New Input -----
# Load original training data to match columns
training_df = pd.read_csv("credit_card_fraud_dataset.csv")
columns_to_drop = ['TransactionID', 'TransactionDate', 'Location', 'IsFraud']
training_df = training_df.drop(columns=columns_to_drop, errors='ignore')
training_encoded = pd.get_dummies(training_df, drop_first=True)

# Encode new input
new_df_encoded = pd.get_dummies(new_df)

# Reindex to match training columns (fill missing columns with 0)
new_df_encoded = new_df_encoded.reindex(columns=training_encoded.columns, fill_value=0)

# ----- Final Encoded Input -----
print("\n‚úÖ Encoded Input Ready for Prediction:")
print(new_df_encoded.head())

"""Predict the Final Grade"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

# Load training data
df = pd.read_csv("credit_card_fraud_dataset.csv")
df = df.drop(columns=['TransactionID', 'TransactionDate', 'Location'], errors='ignore')
df = pd.get_dummies(df, drop_first=True)

# Prepare features and target
X = df.drop(columns=['IsFraud'])
y = df['IsFraud']

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train model
model = LogisticRegression()
model.fit(X_scaled, y)

# ---- Predict Function ----
def predict_final_outcome(user_input_dict):
    # Convert input to DataFrame
    input_df = pd.DataFrame([user_input_dict])

    # One-hot encode
    input_encoded = pd.get_dummies(input_df)

    # Align with training data columns
    input_encoded = input_encoded.reindex(columns=X.columns, fill_value=0)

    # Scale
    input_scaled = scaler.transform(input_encoded)

    # Predict
    prediction = model.predict(input_scaled)[0]
    probability = model.predict_proba(input_scaled)[0][1]

    # Output
    result = "Fraud" if prediction == 1 else "Not Fraud"
    print(f"üß† Prediction: {result}")
    print(f"üìä Probability (Fraud): {probability:.2f}")

# ---- Example Usage ----
sample_input = {
    'Amount': 350.0,
    'TransactionType': 'Online',
    'Gender': 'Female'
    # Add other necessary features based on your dataset
}

predict_final_outcome(sample_input)

"""Deployment-Building an Interactive App"""

# üì¶ Import libraries
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
import ipywidgets as widgets
from IPython.display import display, clear_output

# üì• Load and prepare data
df = pd.read_csv("credit_card_fraud_dataset.csv")
df = df.drop(columns=['TransactionID', 'TransactionDate', 'Location'], errors='ignore')
df = pd.get_dummies(df, drop_first=True)

# üß† Split features and target
X = df.drop(columns=['IsFraud'])
y = df['IsFraud']

# ‚öñÔ∏è Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ü§ñ Train model
model = LogisticRegression()
model.fit(X_scaled, y)

# üéØ Get training feature columns
feature_columns = X.columns

# üìã Create interactive widgets
amount = widgets.FloatText(description='Amount:', value=100.0)
gender = widgets.Dropdown(options=['Male', 'Female'], description='Gender:')
txn_type = widgets.Dropdown(options=['Online', 'In-Store'], description='Type:')
predict_btn = widgets.Button(description="üîç Predict", button_style='primary')

# üéõÔ∏è Display input widgets
display(widgets.VBox([amount, gender, txn_type, predict_btn]))

# üß™ Define prediction behavior
def make_prediction(b):
    clear_output(wait=True)
    display(widgets.VBox([amount, gender, txn_type, predict_btn]))

    # Prepare new input
    new_input = {
        'Amount': amount.value,
        'Gender': gender.value,
        'TransactionType': txn_type.value
    }

    input_df = pd.DataFrame([new_input])
    input_encoded = pd.get_dummies(input_df)
    input_encoded = input_encoded.reindex(columns=feature_columns, fill_value=0)
    input_scaled = scaler.transform(input_encoded)

    # Predict
    prediction = model.predict(input_scaled)[0]
    proba = model.predict_proba(input_scaled)[0][1]

    # Show result
    if prediction == 1:
        print(f"üö® Likely Fraudulent Transaction! (Risk: {proba:.2f})")
    else:
        print(f"‚úÖ Transaction Looks Legitimate. (Risk: {proba:.2f})")

# üîÅ Connect prediction function
predict_btn.on_click(make_prediction)

"""Create a Prediction Function"""

import pandas as pd

def predict_fraud(user_input: dict, model, scaler, feature_columns: list) -> dict:
    """
    Predicts the probability and label of a fraudulent transaction.

    Args:
        user_input (dict): Raw input data for prediction.
        model: Trained machine learning model.
        scaler: Fitted scaler used on training data.
        feature_columns (list): List of features used to train the model.

    Returns:
        dict: {
            'prediction': 0 or 1,
            'probability': float (0 to 1),
            'message': str
        }
    """
    try:
        # Step 1: Convert to DataFrame
        input_df = pd.DataFrame([user_input])

        # Step 2: One-hot encode
        input_encoded = pd.get_dummies(input_df)

        # Step 3: Match training features
        input_encoded = input_encoded.reindex(columns=feature_columns, fill_value=0)

        # Step 4: Scale input
        input_scaled = scaler.transform(input_encoded)

        # Step 5: Make prediction
        prediction = model.predict(input_scaled)[0]
        probability = model.predict_proba(input_scaled)[0][1]

        # Step 6: Build result message
        if prediction == 1:
            msg = f"‚ö†Ô∏è Likely Fraudulent Transaction (Risk Score: {probability:.2f})"
        else:
            msg = f"‚úÖ Transaction Appears Legitimate (Risk Score: {probability:.2f})"

        return {
            'prediction': int(prediction),
            'probability': float(probability),
            'message': msg
        }

    except Exception as e:
        return {
            'prediction': None,
            'probability': None,
            'message': f"Error in prediction: {str(e)}"
        }

pip install gradio

"""Create the Gradio Interface"""

# üì¶ Install and Import libraries
!pip install gradio

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
import gradio as gr

# --- Load and prepare data ---
df = pd.read_csv("credit_card_fraud_dataset.csv")
df = df.drop(columns=['TransactionID', 'TransactionDate', 'Location'], errors='ignore')
df = pd.get_dummies(df, drop_first=True)

# üß† Split features and target
target_column = 'IsFraud' # Define target column explicitly
X = df.drop(columns=[target_column]) # Use the defined target column
y = df[target_column]

# ‚öñÔ∏è Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ü§ñ Train model
model = LogisticRegression()
model.fit(X_scaled, y)

# üéØ Get training feature columns
feature_columns = X.columns.tolist() # Convert to list for safety in the prediction function

# --- Fraud prediction function ---
# Define the prediction function that Gradio will call
def predict_fraud_gradio(amount, gender, transaction_type):
    try:
        # Prepare the input dictionary matching the expected format after one-hot encoding
        # Create a dictionary with the known columns and fill others with 0 initially
        input_dict = {col: 0 for col in feature_columns}

        # Update with the user's input values
        # Handle the specific categorical inputs ('Gender', 'TransactionType') by matching the column names created by get_dummies
        input_dict['Amount'] = amount
        # Check for the one-hot encoded columns based on user input
        # Assumes get_dummies creates columns like 'Gender_Female', 'Gender_Male', 'TransactionType_In-Store', 'TransactionType_Online'
        if f'Gender_{gender}' in feature_columns:
            input_dict[f'Gender_{gender}'] = 1
        if f'TransactionType_{transaction_type}' in feature_columns:
             input_dict[f'TransactionType_{transaction_type}'] = 1

        # Convert dictionary to DataFrame
        # Ensure columns are in the correct order and handle potential missing columns from user input
        input_df = pd.DataFrame([input_dict])
        input_df = input_df.reindex(columns=feature_columns, fill_value=0)


        # Scale input using the fitted scaler
        input_scaled = scaler.transform(input_df) # Transform the DataFrame

        # Make prediction using the trained model
        prediction = model.predict(input_scaled)[0]
        probability = model.predict_proba(input_scaled)[0][1] # Probability of the positive class (Fraud)

        # Format the output message
        if prediction == 1:
            return f"üö® Fraudulent Transaction Likely! (Risk: {probability:.2f})"
        else:
            return f"‚úÖ Transaction Appears Legitimate. (Risk: {probability:.2f})"

    except Exception as e:
        # Return an error message if anything goes wrong during prediction
        return f"‚ùå Error during prediction: {str(e)}"


# --- Gradio UI ---
# Define the Gradio interface
iface = gr.Interface(
    fn=predict_fraud_gradio, # The function Gradio will call
    inputs=[
        gr.Number(label="Amount ($)", value=100.0), # Number input for amount
        gr.Radio(["Male", "Female"], label="Gender"), # Radio buttons for gender
        gr.Dropdown(["Online", "In-Store"], label="Transaction Type") # Dropdown for transaction type
    ],
    outputs=gr.Textbox(label="Prediction Result"), # Textbox to display the output message
    title="üí≥ Credit Card Fraud Detection", # Title of the interface
    description="Enter transaction details below to predict fraud risk." # Description
)

# Launch the app
iface.launch()